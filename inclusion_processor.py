# inclusion_processor.py - åŒ…å«å…³ç³»å¤„ç†æ¨¡å—ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
import pandas as pd
import numpy as np
import psycopg2
from psycopg2 import pool
from psycopg2.extras import execute_values
import logging
import threading
import os
from datetime import datetime
import time

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "stock_db",
    "user": "postgres",
    "password": "shingowolf123"
}

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = "log"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "inclusion_processing.log"), encoding='utf-8')
    ]
)
logger = logging.getLogger("InclusionProcessor")

# æ•°æ®åº“è¿æ¥æ± 
DB_POOL = None

def initialize_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global DB_POOL
    if DB_POOL is None:
        try:
            DB_POOL = psycopg2.pool.ThreadedConnectionPool(
                minconn=20,   # å¢åŠ æœ€å°è¿æ¥æ•°
                maxconn=100,  # å¢åŠ æœ€å¤§è¿æ¥æ•°
                **DB_CONFIG
            )
            logger.info("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {str(e)}")

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL is None:
        initialize_db_pool()
        if DB_POOL is None:
            raise Exception("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥")
    return DB_POOL.getconn()

def put_db_connection(conn):
    """å½’è¿˜æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL and conn:
        try:
            DB_POOL.putconn(conn)
        except Exception as e:
            logger.warning(f"å½’è¿˜æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")

def preprocess_klines_vectorized_optimized(klines):
    """ä¼˜åŒ–çš„å‘é‡åŒ–é¢„å¤„ç†Kçº¿æ•°æ®ï¼Œåˆå¹¶æ‰€æœ‰åŒæ—¥æ•°æ®"""
    if len(klines) <= 1:
        return klines
    
    # ä½¿ç”¨pandasè¿›è¡Œå‘é‡åŒ–å¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    df = pd.DataFrame(klines)
    df['date'] = pd.to_datetime(df['date'])
    
    # æŒ‰æ—¥æœŸåˆ†ç»„å¹¶èšåˆï¼ˆä¼˜åŒ–èšåˆå‡½æ•°ï¼‰
    grouped = df.groupby('date').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).reset_index()
    
    # å‘é‡åŒ–è½¬æ¢å›klinesæ ¼å¼
    processed = grouped.to_dict('records')
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    for record in processed:
        record['open'] = float(record['open'])
        record['high'] = float(record['high'])
        record['low'] = float(record['low'])
        record['close'] = float(record['close'])
        record['volume'] = int(record['volume'])
    
    # æŒ‰æ—¥æœŸæ’åº
    processed.sort(key=lambda x: x['date'])
    return processed

def check_inclusion_vectorized_optimized(highs, lows, dates):
    """ä¼˜åŒ–çš„å‘é‡åŒ–åŒ…å«å…³ç³»æ£€æŸ¥"""
    n = len(highs)
    if n < 2:
        return np.array([]), np.array([]), np.array([])
    
    # å‘é‡åŒ–è®¡ç®—
    prev_highs = highs[:-1]
    prev_lows = lows[:-1]
    curr_highs = highs[1:]
    curr_lows = lows[1:]
    prev_dates = dates[:-1]
    curr_dates = dates[1:]
    
    # è¿‡æ»¤æ‰åŒæ—¥æ•°æ®
    same_date_mask = prev_dates == curr_dates
    valid_mask = ~same_date_mask
    
    # åªå¤„ç†ä¸åŒæ—¥çš„æ•°æ®
    valid_prev_highs = prev_highs[valid_mask]
    valid_prev_lows = prev_lows[valid_mask]
    valid_curr_highs = curr_highs[valid_mask]
    valid_curr_lows = curr_lows[valid_mask]
    valid_indices = np.where(valid_mask)[0]
    
    # å‘ä¸ŠåŒ…å«ï¼šcurråŒ…å«prev
    up_inclusion = (valid_curr_highs >= valid_prev_highs) & (valid_curr_lows <= valid_prev_lows) & \
                   ~((valid_curr_highs == valid_prev_highs) & (valid_curr_lows == valid_prev_lows))
    
    # å‘ä¸‹åŒ…å«ï¼šprevåŒ…å«curr
    down_inclusion = (valid_prev_highs >= valid_curr_highs) & (valid_prev_lows <= valid_curr_lows) & \
                     ~((valid_prev_highs == valid_curr_highs) & (valid_prev_lows == valid_curr_lows))
    
    # è·å–åŒ…å«å…³ç³»çš„ç´¢å¼•
    up_indices = valid_indices[up_inclusion]
    down_indices = valid_indices[down_inclusion]
    up_directions = np.ones(len(up_indices), dtype=int)
    down_directions = -np.ones(len(down_indices), dtype=int)
    
    return up_indices, down_indices, up_directions, down_directions

def merge_klines_vectorized_optimized(k1_high, k1_low, k1_open, k1_close, k1_volume,
                                    k2_high, k2_low, k2_open, k2_close, k2_volume,
                                    k2_date, direction, prev_direction):
    """
    ä¼˜åŒ–çš„å‘é‡åŒ–Kçº¿åˆå¹¶æ–¹æ³•ï¼ˆç¼ è®ºæ ‡å‡†ï¼‰
    """
    merged_high = max(k1_high, k2_high)
    merged_low = min(k1_low, k2_low)
    merged_volume = k1_volume + k2_volume
    
    # æ ¹æ®å‰ä¸€æ ¹Kçº¿çš„èµ°åŠ¿å†³å®šåŒ…å«å¤„ç†æ–¹å¼ï¼ˆç¼ è®ºæ ‡å‡†ï¼‰
    if direction == 1:  # å‘ä¸ŠåŒ…å«(k2åŒ…å«k1)
        if prev_direction == 1:  # å‰ä¸€æ ¹Kçº¿ä¸Šæ¶¨
            # å–é«˜highå’Œé«˜lowï¼ˆé¡ºåŠ¿ï¼‰
            merged_high = max(k1_high, k2_high)
            merged_low = max(k1_low, k2_low)
        else:  # å‰ä¸€æ ¹Kçº¿ä¸‹è·Œ
            # å–ä½highå’Œä½lowï¼ˆé€†åŠ¿ï¼‰
            merged_high = min(k1_high, k2_high)
            merged_low = min(k1_low, k2_low)
    elif direction == -1:  # å‘ä¸‹åŒ…å«(k1åŒ…å«k2)
        if prev_direction == 1:  # å‰ä¸€æ ¹Kçº¿ä¸Šæ¶¨
            # å–ä½highå’Œä½lowï¼ˆé€†åŠ¿ï¼‰
            merged_high = min(k1_high, k2_high)
            merged_low = min(k1_low, k2_low)
        else:  # å‰ä¸€æ ¹Kçº¿ä¸‹è·Œ
            # å–é«˜highå’Œé«˜lowï¼ˆé¡ºåŠ¿ï¼‰
            merged_high = max(k1_high, k2_high)
            merged_low = max(k1_low, k2_low)
    
    return {
        'date': k2_date,
        'open': k2_open,
        'high': merged_high,
        'low': merged_low,
        'close': k2_close,
        'volume': merged_volume
    }

def save_processed_klines_batch_optimized(stock_code, klines, kline_type='processed'):
    """ä¼˜åŒ–çš„æ‰¹é‡ä¿å­˜å¤„ç†åçš„Kçº¿æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # æ‰¹é‡åˆ é™¤æ—§æ•°æ®
            cur.execute("""
                DELETE FROM processed_klines 
                WHERE stock_code = %s AND kline_type = %s
            """, (stock_code, kline_type))
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            if not klines:
                conn.commit()
                return True
            
            # ä½¿ç”¨execute_valuesè¿›è¡Œæ‰¹é‡æ’å…¥ï¼ˆæ›´å¿«ï¼‰
            data_to_insert = []
            for kline in klines:
                data_to_insert.append((
                    stock_code,
                    kline['date'],
                    float(kline['open']),
                    float(kline['high']),
                    float(kline['low']),
                    float(kline['close']),
                    int(kline['volume']),
                    kline_type
                ))
            
            # ä½¿ç”¨execute_valuesè¿›è¡Œæ‰¹é‡æ’å…¥
            insert_sql = """
                INSERT INTO processed_klines (
                    stock_code, date, open, high, low, close, volume, kline_type
                ) VALUES %s
            """
            
            execute_values(cur, insert_sql, data_to_insert, template=None, page_size=1000)
            
            conn.commit()
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(data_to_insert)} æ¡Kçº¿æ•°æ®åˆ°æ•°æ®åº“ï¼Œè‚¡ç¥¨ä»£ç : {stock_code}")
            return True
            
    except Exception as e:
        error_msg = f"æ‰¹é‡ä¿å­˜å¤„ç†åçš„Kçº¿æ•°æ®æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if conn:
            try:
                conn.rollback()
            except:
                pass
        return False
    finally:
        if conn:
            put_db_connection(conn)

def get_stock_data_batch_optimized(stock_code, limit_days=2000):
    """ä¼˜åŒ–çš„æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT date, open, high, low, close, volume 
                FROM daily_data 
                WHERE stock_code = %s 
                ORDER BY date DESC 
                LIMIT %s
            """, (stock_code, limit_days))
            
            rows = cur.fetchall()
            if not rows:
                return None
                
            # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰æ—¥æœŸå‡åºæ’åˆ—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            df = pd.DataFrame(rows, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            return df
            
    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–è‚¡ç¥¨ {stock_code} æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None
    finally:
        if conn:
            put_db_connection(conn)

def process_inclusion_relation_single(stock_code, progress_callback=None, log_callback=None):
    """é«˜åº¦ä¼˜åŒ–çš„å•ä¸ªè‚¡ç¥¨åŒ…å«å…³ç³»å¤„ç†"""
    start_time = time.time()
    
    try:
        # æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®
        df = get_stock_data_batch_optimized(stock_code, 2000)
        if df is None or len(df) == 0:
            error_msg = f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æ•°æ®"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
            
        if log_callback:
            log_callback(f"æ‰¹é‡è·å–åˆ° {len(df)} æ¡Kçº¿æ•°æ®")
        
        # å‘é‡åŒ–é¢„å¤„ç†åŒæ—¥æ•°æ®
        original_count = len(df)
        klines_list = df.to_dict('records')
        klines_list = preprocess_klines_vectorized_optimized(klines_list)
        processed_count = len(klines_list)
        
        if log_callback and processed_count < original_count:
            log_callback(f"å‘é‡åŒ–é¢„å¤„ç†åŒæ—¥æ•°æ®: {original_count} â†’ {processed_count} æ ¹Kçº¿")
        
        # æ‰¹é‡ä¿å­˜åŸå§‹Kçº¿æ•°æ®
        save_start = time.time()
        save_processed_klines_batch_optimized(stock_code, klines_list, 'original')
        save_time = time.time() - save_start
        if log_callback:
            log_callback(f"åŸå§‹æ•°æ®ä¿å­˜è€—æ—¶: {save_time:.2f}ç§’")
        
        if len(klines_list) < 2:
            error_msg = "æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2æ ¹Kçº¿"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
        
        # å‘é‡åŒ–åŒ…å«å…³ç³»å¤„ç†
        processing_start = time.time()
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå‘é‡åŒ–å¤„ç†
        highs = np.array([float(k['high']) for k in klines_list])
        lows = np.array([float(k['low']) for k in klines_list])
        opens = np.array([float(k['open']) for k in klines_list])
        closes = np.array([float(k['close']) for k in klines_list])
        volumes = np.array([int(k['volume']) for k in klines_list])
        dates = np.array([k['date'] for k in klines_list])
        
        # é¢„åˆ†é…ç»“æœæ•°ç»„
        processed_klines = []
        processed_klines.append(klines_list[0])  # ç¬¬ä¸€æ ¹Kçº¿ç›´æ¥æ·»åŠ 
        merge_count = 0
        i = 1
        total_klines = len(klines_list)
        
        # å‘é‡åŒ–å¤„ç†åŒ…å«å…³ç³»
        while i < total_klines:
            current_time = time.time()
            if current_time - start_time > 180:  # 3åˆ†é’Ÿè¶…æ—¶
                if log_callback:
                    log_callback(f"âš ï¸ è‚¡ç¥¨ {stock_code} å¤„ç†è¶…æ—¶")
                break
            
            current_kline = klines_list[i]
            prev_kline = processed_klines[-1]
            
            # è·³è¿‡åŒæ—¥æ•°æ®
            if prev_kline['date'] == current_kline['date']:
                processed_klines.append(current_kline)
                i += 1
                continue
            
            # æ£€æŸ¥åŒ…å«å…³ç³»
            prev_high = float(prev_kline['high'])
            prev_low = float(prev_kline['low'])
            curr_high = float(current_kline['high'])
            curr_low = float(current_kline['low'])
            
            has_inclusion = False
            direction = 0
            
            # å‘ä¸ŠåŒ…å«ï¼šcurrentåŒ…å«prev
            if curr_high >= prev_high and curr_low <= prev_low and not (curr_high == prev_high and curr_low == prev_low):
                has_inclusion = True
                direction = 1
            # å‘ä¸‹åŒ…å«ï¼šprevåŒ…å«current
            elif prev_high >= curr_high and prev_low <= curr_low and not (prev_high == curr_high and prev_low == curr_low):
                has_inclusion = True
                direction = -1
            
            if has_inclusion:
                # ç¡®å®šå‰ä¸€æ ¹Kçº¿çš„æ–¹å‘
                prev_direction = 1
                if len(processed_klines) >= 2:
                    prev_prev_kline = processed_klines[-2]
                    if float(prev_prev_kline['close']) < float(prev_prev_kline['open']):
                        prev_direction = -1
                
                # åˆå¹¶Kçº¿
                merged_kline = merge_klines_vectorized_optimized(
                    prev_high, prev_low, float(prev_kline['open']), float(prev_kline['close']), int(prev_kline['volume']),
                    curr_high, curr_low, float(current_kline['open']), float(current_kline['close']), int(current_kline['volume']),
                    current_kline['date'], direction, prev_direction
                )
                
                # æ›¿æ¢æœ€åä¸€æ ¹Kçº¿
                processed_klines[-1] = merged_kline
                merge_count += 1
                i += 1  # è·³è¿‡å½“å‰Kçº¿
                
                # æ£€æŸ¥è¿ç»­åŒ…å«
                consecutive_merge_count = 0
                while len(processed_klines) >= 2 and consecutive_merge_count < 10:
                    prev_check = processed_klines[-2]
                    curr_check = processed_klines[-1]
                    
                    # è·³è¿‡åŒæ—¥æ•°æ®
                    if prev_check['date'] == curr_check['date']:
                        break
                    
                    prev_check_high = float(prev_check['high'])
                    prev_check_low = float(prev_check['low'])
                    curr_check_high = float(curr_check['high'])
                    curr_check_low = float(curr_check['low'])
                    
                    has_inclusion_again = False
                    direction_again = 0
                    
                    # å‘ä¸ŠåŒ…å«
                    if curr_check_high >= prev_check_high and curr_check_low <= prev_check_low and not (curr_check_high == prev_check_high and curr_check_low == prev_check_low):
                        has_inclusion_again = True
                        direction_again = 1
                    # å‘ä¸‹åŒ…å«
                    elif prev_check_high >= curr_check_high and prev_check_low <= curr_check_low and not (prev_check_high == curr_check_high and prev_check_low == curr_check_low):
                        has_inclusion_again = True
                        direction_again = -1
                    
                    if has_inclusion_again:
                        # ç¡®å®šå‰ä¸€æ ¹Kçº¿çš„æ–¹å‘
                        prev_direction_again = 1
                        if len(processed_klines) >= 3:
                            prev_prev_check = processed_klines[-3]
                            if float(prev_prev_check['close']) < float(prev_prev_check['open']):
                                prev_direction_again = -1
                        
                        # åˆå¹¶Kçº¿
                        merged_kline_again = merge_klines_vectorized_optimized(
                            prev_check_high, prev_check_low, float(prev_check['open']), float(prev_check['close']), int(prev_check['volume']),
                            curr_check_high, curr_check_low, float(curr_check['open']), float(curr_check['close']), int(curr_check['volume']),
                            curr_check['date'], direction_again, prev_direction_again
                        )
                        
                        # æ›¿æ¢å€’æ•°ç¬¬äºŒæ ¹Kçº¿ï¼Œåˆ é™¤æœ€åä¸€æ ¹
                        processed_klines[-2] = merged_kline_again
                        processed_klines.pop()
                        merge_count += 1
                        consecutive_merge_count += 1
                    else:
                        break
            else:
                # æ— åŒ…å«å…³ç³»ï¼Œæ·»åŠ å½“å‰Kçº¿
                processed_klines.append(current_kline)
                i += 1
            
            # æ›´æ–°è¿›åº¦ï¼ˆæ¯å¤„ç†50æ ¹Kçº¿æ›´æ–°ä¸€æ¬¡ï¼‰
            if progress_callback and total_klines > 0 and i % 50 == 0:
                progress = int((min(i, total_klines) / total_klines) * 100)
                progress_callback(progress)
        
        processing_time = time.time() - processing_start
        
        # æ‰¹é‡ä¿å­˜å¤„ç†åçš„Kçº¿æ•°æ®
        save_start = time.time()
        save_result = save_processed_klines_batch_optimized(stock_code, processed_klines, 'processed')
        save_time = time.time() - save_start
        
        if not save_result:
            error_msg = "ä¿å­˜å¤„ç†åæ•°æ®å¤±è´¥"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        processing_speed = total_klines / processing_time if processing_time > 0 else 0
        compression_rate = ((total_klines - len(processed_klines)) / total_klines * 100) if total_klines > 0 else 0
        
        if log_callback:
            log_callback(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            log_callback(f"   åŸå§‹Kçº¿æ•°: {total_klines}")
            log_callback(f"   å¤„ç†åKçº¿æ•°: {len(processed_klines)}")
            log_callback(f"   åˆå¹¶æ¬¡æ•°: {merge_count}")
            log_callback(f"   å‹ç¼©ç‡: {compression_rate:.1f}%")
            log_callback(f"   å¤„ç†é€Ÿåº¦: {processing_speed:.1f} Kçº¿/ç§’")
            log_callback(f"   å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’")
            log_callback(f"   ä¿å­˜è€—æ—¶: {save_time:.2f} ç§’")
            log_callback(f"   æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒæ—¥æ•°æ®
            dates = [k['date'] for k in processed_klines]
            duplicate_dates = []
            for date in dates:
                if dates.count(date) > 1 and date not in duplicate_dates:
                    duplicate_dates.append(date)
            
            if duplicate_dates:
                log_callback(f"   âš ï¸  å‘ç°åŒæ—¥æ•°æ®: {len(duplicate_dates)} ä¸ªæ—¥æœŸ")
            else:
                log_callback(f"   âœ… æ— åŒæ—¥æ•°æ®")
        
        result_msg = f"åŒ…å«å…³ç³»å¤„ç†å®Œæˆï¼ŒåŸå§‹ {total_klines} æ ¹Kçº¿ï¼Œå¤„ç†å {len(processed_klines)} æ ¹Kçº¿ï¼Œåˆå¹¶ {merge_count} æ¬¡"
        if log_callback:
            log_callback("âœ… " + result_msg)
            log_callback("âœ… å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
            
        return True, result_msg
        
    except Exception as e:
        error_msg = f"å¤„ç†åŒ…å«å…³ç³»æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if log_callback:
            log_callback("âŒ " + error_msg)
        return False, error_msg

# åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
if __name__ != "__main__":
    initialize_db_pool()