# chanlun_processor.py - ç¼ è®ºå¤„ç†æ ¸å¿ƒæ¨¡å—ï¼ˆä¿®æ­£ç‰ˆï¼‰
import pandas as pd
import numpy as np
import psycopg2
from psycopg2 import pool
import logging
import threading
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from collections import defaultdict

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "stock_db",
    "user": "postgres",
    "password": "shingowolf123"
}

# å¤„ç†é…ç½®
PROCESSING_CONFIG = {
    'MAX_WORKERS': 4,  # é»˜è®¤çº¿ç¨‹æ•°
    'TIMEOUT_PER_STOCK': 60,  # æ¯åªè‚¡ç¥¨å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
}

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = "log"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "chanlun_processing.log"), encoding='utf-8')
    ]
)
logger = logging.getLogger("ChanlunProcessor")

# æ•°æ®åº“è¿æ¥æ± 
DB_POOL = None
processing_cancelled = False
processing_lock = threading.Lock()

def set_max_workers(num_workers):
    """è®¾ç½®æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°"""
    global PROCESSING_CONFIG
    PROCESSING_CONFIG['MAX_WORKERS'] = max(1, min(num_workers, 16))  # é™åˆ¶åœ¨1-16ä¹‹é—´

def initialize_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global DB_POOL
    try:
        DB_POOL = psycopg2.pool.ThreadedConnectionPool(
            minconn=1,
            maxconn=PROCESSING_CONFIG['MAX_WORKERS'] + 5,
            **DB_CONFIG
        )
        return True
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL is None:
        if not initialize_db_pool():
            raise Exception("æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–")
    return DB_POOL.getconn()

def put_db_connection(conn):
    """å½’è¿˜æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL and conn:
        DB_POOL.putconn(conn)

def is_processing_cancelled():
    """æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ"""
    global processing_cancelled
    with processing_lock:
        return processing_cancelled

def cancel_processing():
    """å–æ¶ˆå¤„ç†è¿‡ç¨‹"""
    global processing_cancelled
    with processing_lock:
        processing_cancelled = True
    logger.info("å¤„ç†å·²è¢«å–æ¶ˆ")

def reset_cancel_flag():
    """é‡ç½®å–æ¶ˆæ ‡å¿—"""
    global processing_cancelled
    with processing_lock:
        processing_cancelled = False
    logger.info("å–æ¶ˆæ ‡å¿—å·²é‡ç½®")

def get_stock_data(stock_code, limit_days=1000):
    """è·å–è‚¡ç¥¨æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT date, open, high, low, close, volume 
                FROM daily_data 
                WHERE stock_code = %s 
                ORDER BY date DESC 
                LIMIT %s
            """, (stock_code, limit_days))
            
            rows = cur.fetchall()
            if not rows:
                return None
                
            # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰æ—¥æœŸå‡åºæ’åˆ—
            df = pd.DataFrame(rows, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            return df
            
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ {stock_code} æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None
    finally:
        if conn:
            put_db_connection(conn)

def get_all_stock_codes():
    """è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç """
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("SELECT DISTINCT stock_code FROM daily_data ORDER BY stock_code")
            rows = cur.fetchall()
            return [row[0] for row in rows] if rows else []
    except Exception as e:
        logger.error(f"è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç æ—¶å‡ºé”™: {str(e)}")
        return []
    finally:
        if conn:
            put_db_connection(conn)

def preprocess_klines_complete(klines):
    """å®Œæ•´é¢„å¤„ç†Kçº¿æ•°æ®ï¼Œåˆå¹¶æ‰€æœ‰åŒæ—¥æ•°æ®"""
    if len(klines) <= 1:
        return klines
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    date_groups = defaultdict(list)
    for kline in klines:
        date_groups[kline['date']].append(kline)
    
    # åˆå¹¶æ¯ç»„åŒæ—¥æ•°æ®
    processed = []
    for date, group in date_groups.items():
        if len(group) == 1:
            processed.append(group[0])
        else:
            # åˆå¹¶åŒæ—¥æ•°æ®
            merged = {
                'date': date,
                'open': group[0]['open'],  # ç¬¬ä¸€æ ¹çš„å¼€ç›˜ä»·
                'high': max([float(k['high']) for k in group]),
                'low': min([float(k['low']) for k in group]),
                'close': group[-1]['close'],  # æœ€åä¸€æ ¹çš„æ”¶ç›˜ä»·
                'volume': sum([int(k['volume']) for k in group])
            }
            processed.append(merged)
    
    # æŒ‰æ—¥æœŸæ’åº
    processed.sort(key=lambda x: x['date'])
    return processed

def check_inclusion(k1, k2):
    """
    æ£€æŸ¥ä¸¤æ ¹Kçº¿æ˜¯å¦åŒ…å«å…³ç³»
    k1, k2: dict, åŒ…å«high, lowå­—æ®µ
    è¿”å›: (æ˜¯å¦åŒ…å«, æ–¹å‘) 
    æ–¹å‘: 1=å‘ä¸ŠåŒ…å«(k2åŒ…å«k1), -1=å‘ä¸‹åŒ…å«(k1åŒ…å«k2), 0=æ— åŒ…å«
    """
    # åŒæ—¥æ•°æ®ä¸è®¤ä¸ºæ˜¯åŒ…å«å…³ç³»
    if k1['date'] == k2['date']:
        return False, 0
    
    k1_high = round(float(k1['high']), 4)
    k1_low = round(float(k1['low']), 4)
    k2_high = round(float(k2['high']), 4)
    k2_low = round(float(k2['low']), 4)
    
    if k1_high >= k2_high and k1_low <= k2_low and not (k1_high == k2_high and k1_low == k2_low):
        return True, -1  # k1åŒ…å«k2ï¼Œå‘ä¸‹åŒ…å«
    elif k2_high >= k1_high and k2_low <= k1_low and not (k2_high == k1_high and k2_low == k1_low):
        return True, 1   # k2åŒ…å«k1ï¼Œå‘ä¸ŠåŒ…å«
    else:
        return False, 0  # æ— åŒ…å«å…³ç³»

def get_kline_direction(kline):
    """åˆ¤æ–­Kçº¿æ–¹å‘ï¼š1=ä¸Šæ¶¨, -1=ä¸‹è·Œ, 0=å¹³ç›˜"""
    if float(kline['close']) > float(kline['open']):
        return 1
    elif float(kline['close']) < float(kline['open']):
        return -1
    else:
        return 0

def merge_klines_correct(k1, k2, direction, prev_direction):
    """
    æ­£ç¡®çš„Kçº¿åˆå¹¶æ–¹æ³•ï¼ˆç¼ è®ºæ ‡å‡†ï¼‰
    direction: 1=å‘ä¸ŠåŒ…å«(k2åŒ…å«k1), -1=å‘ä¸‹åŒ…å«(k1åŒ…å«k2)
    prev_direction: 1=å‰ä¸€æ ¹Kçº¿ä¸Šæ¶¨, -1=å‰ä¸€æ ¹Kçº¿ä¸‹è·Œ
    """
    merged = {
        'date': k2['date'],  # ä¿ç•™è¾ƒæ–°çš„æ—¥æœŸ
        'open': k2['open'],
        'high': max(float(k1['high']), float(k2['high'])),
        'low': min(float(k1['low']), float(k2['low'])),
        'close': k2['close'],
        'volume': int(k1['volume']) + int(k2['volume'])
    }
    
    # æ ¹æ®å‰ä¸€æ ¹Kçº¿çš„èµ°åŠ¿å†³å®šåŒ…å«å¤„ç†æ–¹å¼ï¼ˆç¼ è®ºæ ‡å‡†ï¼‰
    if direction == 1:  # å‘ä¸ŠåŒ…å«(k2åŒ…å«k1)
        if prev_direction == 1:  # å‰ä¸€æ ¹Kçº¿ä¸Šæ¶¨
            # å–é«˜highå’Œé«˜lowï¼ˆé¡ºåŠ¿ï¼‰
            merged['high'] = max(float(k1['high']), float(k2['high']))
            merged['low'] = max(float(k1['low']), float(k2['low']))
        else:  # å‰ä¸€æ ¹Kçº¿ä¸‹è·Œ
            # å–ä½highå’Œä½lowï¼ˆé€†åŠ¿ï¼‰
            merged['high'] = min(float(k1['high']), float(k2['high']))
            merged['low'] = min(float(k1['low']), float(k2['low']))
    elif direction == -1:  # å‘ä¸‹åŒ…å«(k1åŒ…å«k2)
        if prev_direction == 1:  # å‰ä¸€æ ¹Kçº¿ä¸Šæ¶¨
            # å–ä½highå’Œä½lowï¼ˆé€†åŠ¿ï¼‰
            merged['high'] = min(float(k1['high']), float(k2['high']))
            merged['low'] = min(float(k1['low']), float(k2['low']))
        else:  # å‰ä¸€æ ¹Kçº¿ä¸‹è·Œ
            # å–é«˜highå’Œé«˜lowï¼ˆé¡ºåŠ¿ï¼‰
            merged['high'] = max(float(k1['high']), float(k2['high']))
            merged['low'] = max(float(k1['low']), float(k2['low']))
        
    return merged

def save_processed_klines(stock_code, klines, kline_type='processed'):
    """ä¿å­˜å¤„ç†åçš„Kçº¿æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # å…ˆåˆ é™¤æ—§æ•°æ®
            cur.execute("""
                DELETE FROM processed_klines 
                WHERE stock_code = %s AND kline_type = %s
            """, (stock_code, kline_type))
            
            # æ‰¹é‡æ’å…¥æ–°æ•°æ®
            data_to_insert = []
            for kline in klines:
                data_to_insert.append((
                    stock_code,
                    kline['date'],
                    float(kline['open']),
                    float(kline['high']),
                    float(kline['low']),
                    float(kline['close']),
                    int(kline['volume']),
                    kline_type
                ))
            
            cur.executemany("""
                INSERT INTO processed_klines (
                    stock_code, date, open, high, low, close, volume, kline_type
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, data_to_insert)
            
            conn.commit()
            return True
            
    except Exception as e:
        logger.error(f"ä¿å­˜å¤„ç†åçš„Kçº¿æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            put_db_connection(conn)

def get_processed_klines(stock_code, kline_type='processed', limit=100):
    """è·å–å¤„ç†åçš„Kçº¿æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT date, open, high, low, close, volume 
                FROM processed_klines 
                WHERE stock_code = %s AND kline_type = %s
                ORDER BY date DESC 
                LIMIT %s
            """, (stock_code, kline_type, limit))
            
            rows = cur.fetchall()
            if not rows:
                return None
                
            # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰æ—¥æœŸå‡åºæ’åˆ—
            df = pd.DataFrame(rows, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            return df
            
    except Exception as e:
        logger.error(f"è·å–å¤„ç†åçš„Kçº¿æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None
    finally:
        if conn:
            put_db_connection(conn)

def create_tables_if_not_exists():
    """è‡ªåŠ¨åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # åˆ›å»ºæ—¥çº¿æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS daily_data (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    amount DECIMAL,
                    macd DECIMAL,
                    macd_signal DECIMAL,
                    macd_histogram DECIMAL,
                    PRIMARY KEY (stock_code, date)
                )
            """)
            
            # åˆ›å»ºå‘¨çº¿æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS weekly_data (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    amount DECIMAL,
                    macd DECIMAL,
                    macd_signal DECIMAL,
                    macd_histogram DECIMAL,
                    PRIMARY KEY (stock_code, date)
                )
            """)
            
            # åˆ›å»ºæœˆçº¿æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS monthly_data (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    amount DECIMAL,
                    macd DECIMAL,
                    macd_signal DECIMAL,
                    macd_histogram DECIMAL,
                    PRIMARY KEY (stock_code, date)
                )
            """)
            
            # åˆ›å»ºå¤„ç†åçš„Kçº¿æ•°æ®è¡¨ï¼ˆåŒ…å«å…³ç³»å¤„ç†åï¼‰
            cur.execute("""
                CREATE TABLE IF NOT EXISTS processed_klines (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    kline_type VARCHAR(20), -- 'original' æˆ– 'processed'
                    PRIMARY KEY (stock_code, date, kline_type)
                )
            """)
            
            # åˆ›å»ºåˆ†å‹æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS fractals (
                    id SERIAL PRIMARY KEY,
                    stock_code VARCHAR(20),
                    date DATE,
                    fractal_type VARCHAR(10), -- 'top' æˆ– 'bottom'
                    high DECIMAL,
                    low DECIMAL,
                    confirmation_date DATE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
            return True, "æ•°æ®è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆ"
    except Exception as e:
        error_msg = f"åˆ›å»ºæ•°æ®è¡¨æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if conn:
            conn.rollback()
        return False, error_msg
    finally:
        if conn:
            put_db_connection(conn)

def process_inclusion_relation_single(stock_code, progress_callback=None, log_callback=None):
    """å¤„ç†å•ä¸ªè‚¡ç¥¨çš„åŒ…å«å…³ç³»ï¼ˆä¿®æ­£çš„æ­£ç¡®é€»è¾‘ï¼‰"""
    start_time = time.time()
    
    try:
        # è·å–è‚¡ç¥¨æ•°æ®
        df = get_stock_data(stock_code)
        if df is None or len(df) == 0:
            error_msg = f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æ•°æ®"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
            
        if log_callback:
            log_callback(f"è·å–åˆ° {len(df)} æ¡Kçº¿æ•°æ®")
            
        # è½¬æ¢ä¸ºKçº¿åˆ—è¡¨
        klines = []
        for _, row in df.iterrows():
            klines.append({
                'date': row['date'],
                'open': float(row['open']),
                'high': float(row['high']),
                'low': float(row['low']),
                'close': float(row['close']),
                'volume': int(row['volume'])
            })
        
        # å®Œæ•´é¢„å¤„ç†åŒæ—¥æ•°æ®
        original_count = len(klines)
        klines = preprocess_klines_complete(klines)
        if log_callback and len(klines) < original_count:
            log_callback(f"é¢„å¤„ç†åŒæ—¥æ•°æ®: {original_count} â†’ {len(klines)} æ ¹Kçº¿")
        
        # ä¿å­˜åŸå§‹Kçº¿æ•°æ®
        save_processed_klines(stock_code, klines, 'original')
        
        if len(klines) < 2:
            error_msg = "æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2æ ¹Kçº¿"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
            
        # å¤„ç†åŒ…å«å…³ç³»ï¼ˆæ­£ç¡®çš„ç¼ è®ºé€»è¾‘ï¼‰
        processed_klines = []
        i = 0
        total_klines = len(klines)
        merge_count = 0  # æ€»åˆå¹¶æ¬¡æ•°
        
        while i < total_klines and not is_processing_cancelled():
            current_time = time.time()
            if current_time - start_time > PROCESSING_CONFIG['TIMEOUT_PER_STOCK']:
                if log_callback:
                    log_callback(f"âš ï¸ è‚¡ç¥¨ {stock_code} å¤„ç†è¶…æ—¶")
                break
            
            # å¦‚æœæ˜¯ç¬¬ä¸€æ ¹Kçº¿ï¼Œç›´æ¥æ·»åŠ 
            if len(processed_klines) == 0:
                processed_klines.append(klines[i])
                i += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€æ ¹Kçº¿æœ‰åŒ…å«å…³ç³»
            has_inclusion, direction = check_inclusion(processed_klines[-1], klines[i])
            
            if has_inclusion:
                # æœ‰åŒ…å«å…³ç³»ï¼šAå’ŒBåŒ…å«å¤„ç†
                prev_direction = get_kline_direction(processed_klines[-2]) if len(processed_klines) >= 2 else 1
                merged_result = merge_klines_correct(processed_klines[-1], klines[i], direction, prev_direction)
                
                # æ­£ç¡®çš„ä¸¢å¼ƒé€»è¾‘ï¼š
                # 1. ä¸¢å¼ƒAï¼šprocessed_klines[-1]ï¼ˆé€šè¿‡æ›¿æ¢ï¼‰
                # 2. ä¸¢å¼ƒBï¼šklines[i]ï¼ˆé€šè¿‡å¢åŠ iè·³è¿‡ï¼‰
                # 3. ä¿ç•™ç»“æœCï¼šmerged_result
                processed_klines[-1] = merged_result  # ç”¨Cæ›¿æ¢Aï¼ˆä¸¢å¼ƒAï¼‰
                i += 1  # è·³è¿‡Bï¼ˆä¸¢å¼ƒBï¼‰
                
                merge_count += 1
                
                # æ£€æŸ¥åˆå¹¶åçš„Kçº¿æ˜¯å¦è¿˜èƒ½ä¸æ›´å‰é¢çš„Kçº¿åˆå¹¶ï¼ˆè¿ç»­åŒ…å«ï¼‰
                consecutive_merge_count = 0
                while len(processed_klines) >= 2 and consecutive_merge_count < 10:
                    has_inclusion_again, direction_again = check_inclusion(processed_klines[-2], processed_klines[-1])
                    
                    if has_inclusion_again:
                        prev_direction_again = get_kline_direction(processed_klines[-3]) if len(processed_klines) >= 3 else 1
                        merged_result_again = merge_klines_correct(processed_klines[-2], processed_klines[-1], direction_again, prev_direction_again)
                        
                        # è¿ç»­åŒ…å«çš„ä¸¢å¼ƒé€»è¾‘ï¼š
                        # 1. ä¸¢å¼ƒprocessed_klines[-2]å’Œprocessed_klines[-1]
                        # 2. ä¿ç•™åˆå¹¶ç»“æœmerged_result_again
                        processed_klines.pop()  # ä¸¢å¼ƒæœ€åä¸€æ ¹
                        processed_klines[-1] = merged_result_again  # ç”¨åˆå¹¶ç»“æœæ›¿æ¢å€’æ•°ç¬¬ä¸€æ ¹
                        
                        merge_count += 1
                        consecutive_merge_count += 1
                    else:
                        break
            else:
                # æ— åŒ…å«å…³ç³»ï¼Œæ·»åŠ å½“å‰Kçº¿
                processed_klines.append(klines[i])
                i += 1
            
            # æ›´æ–°è¿›åº¦ï¼ˆæ¯å¤„ç†10æ ¹Kçº¿æ›´æ–°ä¸€æ¬¡ï¼‰
            if progress_callback and total_klines > 0 and i % 10 == 0:
                progress = int((min(i, total_klines) / total_klines) * 100)
                progress_callback(progress)
            
            # æ£€æŸ¥å–æ¶ˆæ ‡å¿—
            if is_processing_cancelled():
                if log_callback:
                    log_callback(f"âš ï¸ è‚¡ç¥¨ {stock_code} å¤„ç†è¢«å–æ¶ˆ")
                return False, "å¤„ç†è¢«å–æ¶ˆ"
        
        if is_processing_cancelled():
            return False, "å¤„ç†è¢«å–æ¶ˆ"
        
        # ä¿å­˜å¤„ç†åçš„Kçº¿æ•°æ®
        save_processed_klines(stock_code, processed_klines, 'processed')
        
        # æ·»åŠ è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        compression_rate = ((len(klines) - len(processed_klines)) / len(klines) * 100) if len(klines) > 0 else 0
        
        if log_callback:
            log_callback(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            log_callback(f"   åŸå§‹Kçº¿æ•°: {len(klines)}")
            log_callback(f"   å¤„ç†åKçº¿æ•°: {len(processed_klines)}")
            log_callback(f"   åˆå¹¶æ¬¡æ•°: {merge_count}")
            log_callback(f"   å‹ç¼©ç‡: {compression_rate:.1f}%")
            log_callback(f"   å¤„ç†è€—æ—¶: {elapsed_time:.2f} ç§’")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒæ—¥æ•°æ®
            dates = [k['date'] for k in processed_klines]
            duplicate_dates = []
            for date in dates:
                if dates.count(date) > 1 and date not in duplicate_dates:
                    duplicate_dates.append(date)
            
            if duplicate_dates:
                log_callback(f"   âš ï¸  å‘ç°åŒæ—¥æ•°æ®: {len(duplicate_dates)} ä¸ªæ—¥æœŸ")
            else:
                log_callback(f"   âœ… æ— åŒæ—¥æ•°æ®")
        
        result_msg = f"åŒ…å«å…³ç³»å¤„ç†å®Œæˆï¼ŒåŸå§‹ {len(klines)} æ ¹Kçº¿ï¼Œå¤„ç†å {len(processed_klines)} æ ¹Kçº¿ï¼Œåˆå¹¶ {merge_count} æ¬¡"
        if log_callback:
            log_callback("âœ… " + result_msg)
            log_callback("âœ… å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
            
        return True, result_msg
        
    except Exception as e:
        error_msg = f"å¤„ç†åŒ…å«å…³ç³»æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if log_callback:
            log_callback("âŒ " + error_msg)
        return False, error_msg

def process_inclusion_relation(stock_code, progress_callback=None, log_callback=None):
    """å¤„ç†åŒ…å«å…³ç³»ï¼ˆåŒ…è£…å‡½æ•°ï¼Œæ”¯æŒå–æ¶ˆæ£€æŸ¥ï¼‰"""
    if is_processing_cancelled():
        return False, "å¤„ç†è¢«å–æ¶ˆ"
    
    return process_inclusion_relation_single(stock_code, progress_callback, log_callback)

def process_single_stock_wrapper(stock_code, process_type, log_callback=None):
    """å•ä¸ªè‚¡ç¥¨å¤„ç†åŒ…è£…å‡½æ•°ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰"""
    try:
        if process_type == "inclusion":
            success, message = process_inclusion_relation(stock_code, log_callback=log_callback)
        elif process_type == "fractal":
            success, message = process_fractals(stock_code, log_callback=log_callback)
        else:
            success = False
            message = f"æœªçŸ¥çš„å¤„ç†ç±»å‹: {process_type}"
        
        return stock_code, success, message
    except Exception as e:
        return stock_code, False, f"å¤„ç†å¼‚å¸¸: {str(e)}"

def batch_process_all_stocks(process_type, progress_callback=None, log_callback=None):
    """æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰- å…¨é‡æ›´æ–°"""
    reset_cancel_flag()
    
    if log_callback:
        log_callback(f"å¼€å§‹æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨çš„{process_type}ï¼ˆ{PROCESSING_CONFIG['MAX_WORKERS']}çº¿ç¨‹ï¼‰...")
    
    try:
        # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
        stock_codes = get_all_stock_codes()
        if not stock_codes:
            error_msg = "æœªæ‰¾åˆ°ä»»ä½•è‚¡ç¥¨æ•°æ®"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
            
        if log_callback:
            log_callback(f"å…±æ‰¾åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
            
        total_stocks = len(stock_codes)
        successful = 0
        failed_stocks = []
        completed = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç† - å¤„ç†æ‰€æœ‰è‚¡ç¥¨
        with ThreadPoolExecutor(max_workers=PROCESSING_CONFIG['MAX_WORKERS']) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡ - å¤„ç†æ‰€æœ‰è‚¡ç¥¨
            future_to_stock = {
                executor.submit(process_single_stock_wrapper, stock_code, process_type): stock_code 
                for stock_code in stock_codes  # å¤„ç†æ‰€æœ‰è‚¡ç¥¨
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_stock):
                if is_processing_cancelled():
                    if log_callback:
                        log_callback("âš ï¸ å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ")
                    # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                    for f in future_to_stock:
                        f.cancel()
                    break
                
                try:
                    stock_code, success, message = future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
                    
                    if success:
                        successful += 1
                        if log_callback and successful % 10 == 0:  # æ¯10ä¸ªæˆåŠŸè®°å½•ä¸€æ¬¡
                            progress_percent = (successful / total_stocks) * 100
                            log_callback(f"âœ… å·²å®Œæˆ {successful}/{total_stocks} åªè‚¡ç¥¨ ({progress_percent:.1f}%)")
                    else:
                        failed_stocks.append((stock_code, message))
                    
                    completed += 1
                    
                    # æ›´æ–°è¿›åº¦
                    if progress_callback and total_stocks > 0:
                        progress = int((completed / total_stocks) * 100)
                        progress_callback(progress)
                        
                except Exception as e:
                    stock_code = future_to_stock[future]
                    error_msg = f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}"
                    failed_stocks.append((stock_code, error_msg))
                    if log_callback:
                        log_callback("âŒ " + error_msg)
        
        # ç»Ÿè®¡ç»“æœ - æ˜¾ç¤ºå®Œæ•´ç»Ÿè®¡
        result_msg = f"æ‰¹é‡å¤„ç†å®Œæˆ! æ€»æ•°: {total_stocks}, æˆåŠŸ: {successful}, å¤±è´¥: {len(failed_stocks)}"
        if log_callback:
            log_callback("âœ… " + result_msg)
            if failed_stocks:
                log_callback("âŒ å¤±è´¥è‚¡ç¥¨:")
                for code, error in failed_stocks[:10]:  # æ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                    log_callback(f"  - {code}: {error}")
                if len(failed_stocks) > 10:
                    log_callback(f"  ... è¿˜æœ‰ {len(failed_stocks) - 10} ä¸ªå¤±è´¥")
        
        return not is_processing_cancelled(), result_msg
        
    except Exception as e:
        error_msg = f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        if log_callback:
            log_callback("âŒ " + error_msg)
        return False, error_msg

def process_fractals(stock_code, progress_callback=None, log_callback=None):
    """åˆ†å‹è¯†åˆ«ï¼ˆå¾…å®ç°ï¼‰"""
    if log_callback:
        log_callback("åˆ†å‹è¯†åˆ«åŠŸèƒ½å°šæœªå®ç°")
    return False, "åˆ†å‹è¯†åˆ«åŠŸèƒ½å°šæœªå®ç°"

# åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
if __name__ != "__main__":
    initialize_db_pool()
    create_tables_if_not_exists()