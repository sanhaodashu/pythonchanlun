# fractal_processor.py - åˆ†å‹è¯†åˆ«å¤„ç†æ¨¡å—ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
import pandas as pd
import psycopg2
from psycopg2 import pool
import logging
import threading
from datetime import datetime
import time
import numpy as np

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "stock_db",
    "user": "postgres",
    "password": "shingowolf123"
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("log/fractal_processing.log", encoding='utf-8')
    ]
)
logger = logging.getLogger("FractalProcessor")

# æ•°æ®åº“è¿æ¥æ± 
DB_POOL = None

def initialize_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global DB_POOL
    if DB_POOL is None:
        try:
            DB_POOL = psycopg2.pool.ThreadedConnectionPool(
                minconn=20,   # å¢åŠ æœ€å°è¿æ¥æ•°
                maxconn=100,  # å¢åŠ æœ€å¤§è¿æ¥æ•°
                **DB_CONFIG
            )
            logger.info("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {str(e)}")

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL is None:
        initialize_db_pool()
        if DB_POOL is None:
            raise Exception("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥")
    return DB_POOL.getconn()

def put_db_connection(conn):
    """å½’è¿˜æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL and conn:
        try:
            DB_POOL.putconn(conn)
        except Exception as e:
            logger.warning(f"å½’è¿˜æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")

def get_stock_data_batch(stock_code, limit_days=2000):
    """æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT date, open, high, low, close, volume 
                FROM daily_data 
                WHERE stock_code = %s 
                ORDER BY date DESC 
                LIMIT %s
            """, (stock_code, limit_days))
            
            rows = cur.fetchall()
            if not rows:
                return None
                
            # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰æ—¥æœŸå‡åºæ’åˆ—
            df = pd.DataFrame(rows, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            return df
            
    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–è‚¡ç¥¨ {stock_code} æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None
    finally:
        if conn:
            put_db_connection(conn)

def get_processed_klines_batch(stock_code, kline_type='processed', limit=2000):
    """æ‰¹é‡è·å–å¤„ç†åçš„Kçº¿æ•°æ®"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT date, open, high, low, close, volume 
                FROM processed_klines 
                WHERE stock_code = %s AND kline_type = %s
                ORDER BY date DESC 
                LIMIT %s
            """, (stock_code, kline_type, limit))
            
            rows = cur.fetchall()
            if not rows:
                return None
                
            # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰æ—¥æœŸå‡åºæ’åˆ—
            df = pd.DataFrame(rows, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            return df
            
    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–å¤„ç†åçš„Kçº¿æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None
    finally:
        if conn:
            put_db_connection(conn)

# å‘é‡åŒ–åˆ†å‹æ£€æµ‹å‡½æ•°
def is_top_fractal_vectorized(highs, lows):
    """å‘é‡åŒ–é¡¶åˆ†å‹æ£€æµ‹"""
    n = len(highs)
    if n < 3:
        return np.zeros(n, dtype=bool)
    
    # åˆ›å»ºç»“æœæ•°ç»„
    is_top = np.zeros(n, dtype=bool)
    
    # å‘é‡åŒ–æ¯”è¾ƒï¼ˆåªæ£€æŸ¥ä¸­é—´çš„ç‚¹ï¼‰
    middle_indices = np.arange(1, n-1)
    left_highs = highs[middle_indices - 1]
    middle_highs = highs[middle_indices]
    right_highs = highs[middle_indices + 1]
    left_lows = lows[middle_indices - 1]
    middle_lows = lows[middle_indices]
    right_lows = lows[middle_indices + 1]
    
    # é¡¶åˆ†å‹æ¡ä»¶
    contained = (middle_highs >= left_highs) & (middle_highs >= right_highs) & \
                (middle_lows >= left_lows) & (middle_lows >= right_lows)
    
    not_equal = (middle_highs > left_highs) | (middle_highs > right_highs) | \
                (middle_lows > left_lows) | (middle_lows > right_lows)
    
    is_top[middle_indices] = contained & not_equal
    return is_top

def is_bottom_fractal_vectorized(highs, lows):
    """å‘é‡åŒ–åº•åˆ†å‹æ£€æµ‹"""
    n = len(highs)
    if n < 3:
        return np.zeros(n, dtype=bool)
    
    # åˆ›å»ºç»“æœæ•°ç»„
    is_bottom = np.zeros(n, dtype=bool)
    
    # å‘é‡åŒ–æ¯”è¾ƒï¼ˆåªæ£€æŸ¥ä¸­é—´çš„ç‚¹ï¼‰
    middle_indices = np.arange(1, n-1)
    left_highs = highs[middle_indices - 1]
    middle_highs = highs[middle_indices]
    right_highs = highs[middle_indices + 1]
    left_lows = lows[middle_indices - 1]
    middle_lows = lows[middle_indices]
    right_lows = lows[middle_indices + 1]
    
    # åº•åˆ†å‹æ¡ä»¶
    contained = (middle_highs <= left_highs) & (middle_highs <= right_highs) & \
                (middle_lows <= left_lows) & (middle_lows <= right_lows)
    
    not_equal = (middle_highs < left_highs) | (middle_highs < right_highs) | \
                (middle_lows < left_lows) | (middle_lows < right_lows)
    
    is_bottom[middle_indices] = contained & not_equal
    return is_bottom

def detect_fractals_optimized(klines_df):
    """ä¼˜åŒ–çš„åˆ†å‹æ£€æµ‹ï¼ˆå‘é‡åŒ–+æ­£ç¡®é€»è¾‘ï¼‰"""
    if len(klines_df) < 3:
        return []
    
    # å‘é‡åŒ–å¤„ç†
    highs = klines_df['high'].values.astype(np.float64)
    lows = klines_df['low'].values.astype(np.float64)
    dates = klines_df['date'].values
    
    # å‘é‡åŒ–æ£€æµ‹æ‰€æœ‰å¯èƒ½çš„åˆ†å‹
    is_top = is_top_fractal_vectorized(highs, lows)
    is_bottom = is_bottom_fractal_vectorized(highs, lows)
    
    # ä½¿ç”¨è´ªå¿ƒç®—æ³•ç¡®ä¿åˆ†å‹ä¸é‡å ä¸”æœ‰é—´éš”
    fractals = []
    i = 1
    last_fractal_end_index = -1
    
    while i < len(highs) - 1:
        if i <= last_fractal_end_index:
            i += 1
            continue
            
        if is_top[i]:
            fractal = {
                'date': dates[i],
                'type': 'top',
                'high': float(highs[i]),
                'low': float(lows[i]),
                'index': i
            }
            fractals.append(fractal)
            last_fractal_end_index = i + 1  # åˆ†å‹å ç”¨åˆ°i+1ä½ç½®
            i += 4  # è·³è¿‡è¢«å ç”¨çš„Kçº¿å’Œé—´éš”Kçº¿
        elif is_bottom[i]:
            fractal = {
                'date': dates[i],
                'type': 'bottom',
                'high': float(highs[i]),
                'low': float(lows[i]),
                'index': i
            }
            fractals.append(fractal)
            last_fractal_end_index = i + 1  # åˆ†å‹å ç”¨åˆ°i+1ä½ç½®
            i += 4  # è·³è¿‡è¢«å ç”¨çš„Kçº¿å’Œé—´éš”Kçº¿
        else:
            i += 1
    
    return fractals

def save_fractals_batch_optimized(stock_code, fractals):
    """ä¼˜åŒ–çš„æ‰¹é‡ä¿å­˜åˆ†å‹åˆ°æ•°æ®åº“"""
    conn = None
    try:
        conn = get_db_connection()
        if not conn:
            logger.error(f"âŒ æ— æ³•è·å–æ•°æ®åº“è¿æ¥ï¼Œè‚¡ç¥¨: {stock_code}")
            return False
            
        with conn.cursor() as cur:
            # å…ˆæ¸…ç©ºè¯¥è‚¡ç¥¨çš„æ—§åˆ†å‹æ•°æ®
            cur.execute("""
                DELETE FROM fractals WHERE stock_code = %s
            """, (stock_code,))
            
            # å¦‚æœæ²¡æœ‰åˆ†å‹æ•°æ®ï¼Œç›´æ¥æäº¤
            if not fractals:
                conn.commit()
                return True
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®ï¼ˆä½¿ç”¨executemanyçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            data_to_insert = []
            for fractal in fractals:
                try:
                    stock_code_safe = str(stock_code)[:20]
                    # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼åŒ–æ—¥æœŸï¼Œé¿å…ç±»å‹è½¬æ¢å¼€é”€
                    date_str = fractal['date'].strftime('%Y-%m-%d') if hasattr(fractal['date'], 'strftime') else str(fractal['date'])[:10]
                    fractal_type_safe = str(fractal['type'])[:10]
                    high_safe = float(fractal['high'])
                    low_safe = float(fractal['low'])
                    confirmation_date_str = date_str
                    
                    data_to_insert.append((
                        stock_code_safe,
                        date_str,
                        fractal_type_safe,
                        high_safe,
                        low_safe,
                        confirmation_date_str
                    ))
                except Exception as convert_error:
                    logger.warning(f"æ•°æ®è½¬æ¢è­¦å‘Šï¼Œè‚¡ç¥¨ {stock_code}: {str(convert_error)}")
                    continue
            
            if not data_to_insert:
                conn.commit()
                return True
            
            # ä½¿ç”¨execute_valuesè¿›è¡Œæ‰¹é‡æ’å…¥ï¼ˆæ›´å¿«ï¼‰
            from psycopg2.extras import execute_values
            insert_sql = """
                INSERT INTO fractals (stock_code, date, fractal_type, high, low, confirmation_date)
                VALUES %s
            """
            
            execute_values(cur, insert_sql, data_to_insert, template=None, page_size=100)
            
            conn.commit()
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(data_to_insert)} ä¸ªåˆ†å‹åˆ°æ•°æ®åº“ï¼Œè‚¡ç¥¨ä»£ç : {stock_code}")
            return True
            
    except Exception as e:
        error_msg = f"æ‰¹é‡ä¿å­˜åˆ†å‹åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}"
        logger.error(f"âŒ è‚¡ç¥¨ {stock_code} ä¿å­˜åˆ†å‹æ—¶å‡ºé”™: {error_msg}")
        if conn:
            try:
                conn.rollback()
            except:
                pass
        return False
    finally:
        if conn:
            put_db_connection(conn)

def process_fractals_single(stock_code, progress_callback=None, log_callback=None):
    """ä¼˜åŒ–çš„å•ä¸ªè‚¡ç¥¨åˆ†å‹è¯†åˆ«"""
    start_time = time.time()
    
    try:
        # æ‰¹é‡è·å–æ‰€æœ‰æ•°æ®
        df = get_processed_klines_batch(stock_code, 'processed', 2000)
        if df is None or len(df) == 0:
            # å¦‚æœæ²¡æœ‰å¤„ç†åçš„æ•°æ®ï¼Œå°è¯•è·å–åŸå§‹æ•°æ®
            df = get_processed_klines_batch(stock_code, 'original', 2000)
            if df is None or len(df) == 0:
                # å¦‚æœè¿˜æ²¡æœ‰æ•°æ®ï¼Œä»daily_dataè·å–
                df = get_stock_data_batch(stock_code, 2000)
                if df is None or len(df) == 0:
                    error_msg = f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æ•°æ®"
                    if log_callback:
                        log_callback("âŒ " + error_msg)
                    return False, error_msg
        
        if log_callback:
            log_callback(f"è·å–åˆ° {len(df)} æ¡Kçº¿æ•°æ®")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–åˆ†å‹æ£€æµ‹
        if log_callback:
            log_callback(f"å¼€å§‹ä¼˜åŒ–çš„åˆ†å‹æ£€æµ‹...")
        fractals = detect_fractals_optimized(df)
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡ä¿å­˜
        if fractals:
            save_start = time.time()
            save_result = save_fractals_batch_optimized(stock_code, fractals)
            save_time = time.time() - save_start
            if log_callback:
                log_callback(f"æ•°æ®åº“ä¿å­˜è€—æ—¶: {save_time:.2f}ç§’")
            
            if save_result:
                if log_callback:
                    log_callback(f"âœ… æˆåŠŸä¿å­˜ {len(fractals)} ä¸ªåˆ†å‹åˆ°æ•°æ®åº“")
            else:
                if log_callback:
                    log_callback(f"âŒ æ‰¹é‡ä¿å­˜åˆ†å‹åˆ°æ•°æ®åº“å¤±è´¥")
                return False, "æ‰¹é‡ä¿å­˜åˆ†å‹åˆ°æ•°æ®åº“å¤±è´¥"
        else:
            if log_callback:
                log_callback(f"âš ï¸ æœªæ£€æµ‹åˆ°åˆ†å‹")
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        processing_speed = len(df) / elapsed_time if elapsed_time > 0 else 0
        
        if log_callback:
            log_callback(f"ğŸ“Š åˆ†å‹æ£€æµ‹å®Œæˆ:")
            log_callback(f"   æ€»Kçº¿æ•°: {len(df)}")
            log_callback(f"   æ£€æµ‹åˆ°åˆ†å‹: {len(fractals)} ä¸ª")
            log_callback(f"   å¤„ç†é€Ÿåº¦: {processing_speed:.1f} Kçº¿/ç§’")
            log_callback(f"   æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            log_callback("âœ… åˆ†å‹è¯†åˆ«å®Œæˆ")
        
        result_msg = f"åˆ†å‹è¯†åˆ«å®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(fractals)} ä¸ªåˆ†å‹"
        return True, result_msg
        
    except Exception as e:
        error_msg = f"å¤„ç†åˆ†å‹è¯†åˆ«æ—¶å‡ºé”™: {str(e)}"
        logger.error(f"è‚¡ç¥¨ {stock_code} å¤„ç†åˆ†å‹è¯†åˆ«æ—¶å‡ºé”™: {error_msg}")
        if log_callback:
            log_callback("âŒ " + error_msg)
        return False, error_msg

# åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
if __name__ != "__main__":
    initialize_db_pool()