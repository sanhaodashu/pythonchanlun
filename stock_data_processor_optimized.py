# stock_data_processor_optimized.py - ä¼˜åŒ–çš„è‚¡ç¥¨æ•°æ®å¤„ç†è„šæœ¬
import os
import re
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import psycopg2
from psycopg2 import pool
import logging
import gc
import time
import threading

# ç¡®ä¿logç›®å½•å­˜åœ¨
log_dir = "log"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# å°è¯•å¯¼å…¥pandas_taï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨numpyå®ç°
try:
    import pandas_ta as ta
    USE_PANDAS_TA = True
except ImportError:
    USE_PANDAS_TA = False
    print("æœªå®‰è£…pandas_taåº“ï¼Œå°†ä½¿ç”¨numpyå®ç°MACDè®¡ç®—")

# =============================================================================
# ğŸ› ï¸ é…ç½®åŒºåŸŸ
# =============================================================================

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "stock_db",
    "user": "postgres",
    "password": "shingowolf123"
}

# å¤„ç†å‚æ•°é…ç½®
PROCESSING_CONFIG = {
    'MAX_WORKERS': 8,  # å‡å°‘çº¿ç¨‹æ•°é¿å…å¡æ­»
    'BATCH_INSERT_SIZE': 1000,
    'KEEP_DAYS': 3650,
    'MEMORY_CLEANUP_INTERVAL': 50
}

# MACDå‚æ•°
MACD_PARAMS = {
    'fast': 12,
    'slow': 26, 
    'signal': 9
}

# æ—¥å¿—é…ç½® - åªè¾“å‡ºåˆ°æ–‡ä»¶ï¼Œå‡å°‘æ§åˆ¶å°è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "stock_processing.log"), encoding='utf-8')
    ]
)
logger = logging.getLogger("StockProcessor")

# =============================================================================
# ğŸ—„ï¸ æ•°æ®åº“è¿æ¥æ± å’Œå…¨å±€å˜é‡
# =============================================================================

DB_POOL = None
processing_cancelled = False
processing_lock = threading.Lock()

def initialize_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global DB_POOL
    try:
        DB_POOL = psycopg2.pool.ThreadedConnectionPool(
            minconn=1,
            maxconn=PROCESSING_CONFIG['MAX_WORKERS'] + 2,
            **DB_CONFIG
        )
        return True
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL is None:
        if not initialize_db_pool():
            raise Exception("æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–")
    return DB_POOL.getconn()

def put_db_connection(conn):
    """å½’è¿˜æ•°æ®åº“è¿æ¥"""
    global DB_POOL
    if DB_POOL and conn:
        DB_POOL.putconn(conn)

def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("SELECT version()")
            version = cur.fetchone()
            put_db_connection(conn)
            return True, f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {version[0] if version else 'Unknown'}"
    except Exception as e:
        return False, f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}"

def create_tables_if_not_exists():
    """è‡ªåŠ¨åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # åˆ›å»ºæ—¥çº¿æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS daily_data (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    amount DECIMAL,
                    macd DECIMAL,
                    macd_signal DECIMAL,
                    macd_histogram DECIMAL,
                    PRIMARY KEY (stock_code, date)
                )
            """)
            
            # åˆ›å»ºå‘¨çº¿æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS weekly_data (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    amount DECIMAL,
                    macd DECIMAL,
                    macd_signal DECIMAL,
                    macd_histogram DECIMAL,
                    PRIMARY KEY (stock_code, date)
                )
            """)
            
            # åˆ›å»ºæœˆçº¿æ•°æ®è¡¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS monthly_data (
                    stock_code VARCHAR(20),
                    date DATE,
                    open DECIMAL,
                    high DECIMAL,
                    low DECIMAL,
                    close DECIMAL,
                    volume BIGINT,
                    amount DECIMAL,
                    macd DECIMAL,
                    macd_signal DECIMAL,
                    macd_histogram DECIMAL,
                    PRIMARY KEY (stock_code, date)
                )
            """)
            
            conn.commit()
            return True, "æ•°æ®è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆ"
    except Exception as e:
        error_msg = f"åˆ›å»ºæ•°æ®è¡¨æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if conn:
            conn.rollback()
        return False, error_msg
    finally:
        if conn:
            put_db_connection(conn)

# =============================================================================
# ğŸ“ˆ MACDè®¡ç®—å‡½æ•°
# =============================================================================

def calculate_macd(series, fast=12, slow=26, signal=9):
    """è®¡ç®—MACDæŒ‡æ ‡"""
    if USE_PANDAS_TA and 'ta' in globals():
        # ä½¿ç”¨pandas_taåº“è®¡ç®—
        try:
            macd_result = ta.macd(series, fast=fast, slow=slow, signal=signal)
            if macd_result is not None and len(macd_result.columns) >= 3:
                return macd_result.iloc[:, 0], macd_result.iloc[:, 1], macd_result.iloc[:, 2]  # MACD, Signal, Histogram
        except:
            pass
    
    # ä½¿ç”¨numpyå®ç°MACDè®¡ç®—
    exp1 = series.ewm(span=fast, adjust=False).mean()
    exp2 = series.ewm(span=slow, adjust=False).mean()
    macd = exp1 - exp2
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    histogram = macd - signal_line
    
    return macd, signal_line, histogram

# =============================================================================
# ğŸ”„ æ•°æ®å¤„ç†å‡½æ•°
# =============================================================================

def is_processing_cancelled():
    """æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ"""
    global processing_cancelled
    with processing_lock:
        return processing_cancelled

def cancel_processing():
    """å–æ¶ˆå¤„ç†è¿‡ç¨‹"""
    global processing_cancelled
    with processing_lock:
        processing_cancelled = True

def reset_cancel_flag():
    """é‡ç½®å–æ¶ˆæ ‡å¿—"""
    global processing_cancelled
    with processing_lock:
        processing_cancelled = False

def safe_float_convert(value, default=0.0):
    """å®‰å…¨çš„æµ®ç‚¹æ•°è½¬æ¢"""
    try:
        if pd.isna(value):
            return default
        return float(value)
    except:
        return default

def safe_int_convert(value, default=0):
    """å®‰å…¨çš„æ•´æ•°è½¬æ¢"""
    try:
        if pd.isna(value):
            return default
        return int(float(value))
    except:
        return default

def process_txt_file(txt_path, last_db_date=None):
    """å¤„ç†å•ä¸ªTXTæ–‡ä»¶ï¼Œç”Ÿæˆæ—¥çº¿ã€å‘¨çº¿ã€æœˆçº¿æ•°æ®"""
    if is_processing_cancelled():
        return None, None, None
        
    try:
        # 1. è¯»å–å’Œè§£æTXTæ–‡ä»¶
        data = []
        cutoff_date = datetime.now() - timedelta(days=PROCESSING_CONFIG['KEEP_DAYS']) if PROCESSING_CONFIG['KEEP_DAYS'] > 0 else datetime(1900, 1, 1)
        
        with open(txt_path, 'r', encoding='gbk') as file:
            next(file)  # è·³è¿‡æ ‡é¢˜è¡Œ
            line_count = 0
            for line in file:
                line_count += 1
                if line_count % 1000 == 0 and is_processing_cancelled():  # æ¯1000è¡Œæ£€æŸ¥ä¸€æ¬¡å–æ¶ˆ
                    return None, None, None
                    
                match = re.match(
                    r'(\d{4}/\d{2}/\d{2})[\t\s]+([\d\.]+)[\t\s]+([\d\.]+)[\t\s]+([\d\.]+)[\t\s]+([\d\.]+)[\t\s]+([\d\.]+)[\t\s]+([\d\.]+)',
                    line.strip()
                )
                if match:
                    date_str = match.group(1)
                    file_date = datetime.strptime(date_str, '%Y/%m/%d')
                    # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œåªå¤„ç†æ¯”æ•°æ®åº“ä¸­æ›´æ–°çš„æ•°æ®
                    if last_db_date is not None:
                        if file_date <= last_db_date:
                            continue
                    if file_date >= cutoff_date:
                        data.append({
                            'date': date_str,
                            'open': safe_float_convert(match.group(2)),
                            'high': safe_float_convert(match.group(3)),
                            'low': safe_float_convert(match.group(4)),
                            'close': safe_float_convert(match.group(5)),
                            'volume': safe_int_convert(match.group(6)),
                            'amount': safe_float_convert(match.group(7))
                        })
        
        if not data:  # ä¿®å¤è¯­æ³•é”™è¯¯
            return None, None, None
            
        # 2. è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        df = pd.DataFrame(data).sort_values('date')
        df['date'] = pd.to_datetime(df['date'])
        df = df.set_index('date')
        
        # 3. è®¡ç®—MACDï¼ˆåªåœ¨æœ‰æ–°æ•°æ®æ—¶è®¡ç®—ï¼‰
        if len(df) > 0 and not is_processing_cancelled():
            macd, signal, histogram = calculate_macd(df['close'], 
                                                   MACD_PARAMS['fast'], 
                                                   MACD_PARAMS['slow'], 
                                                   MACD_PARAMS['signal'])
            df['macd'] = macd
            df['macd_signal'] = signal
            df['macd_histogram'] = histogram
        
        if is_processing_cancelled():
            return None, None, None
            
        # 4. å¤„ç†æ—¥çº¿æ•°æ®
        daily_df = df.copy()
        daily_df.reset_index(inplace=True)
        
        # 5. å¤„ç†å‘¨çº¿æ•°æ®
        weekly_df = resample_data(df, 'W-FRI')
        weekly_df.reset_index(inplace=True)
        
        # 6. å¤„ç†æœˆçº¿æ•°æ®
        monthly_df = resample_data(df, 'ME')  # ä¿®å¤è­¦å‘Šï¼šä½¿ç”¨'ME'æ›¿ä»£'M'
        monthly_df.reset_index(inplace=True)
        
        return daily_df, weekly_df, monthly_df
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶ {txt_path} æ—¶å‡ºé”™: {str(e)}")
        return None, None, None

def resample_data(df, freq):
    """å‘¨æœŸè½¬æ¢ï¼ˆæ—¥çº¿->å‘¨çº¿/æœˆçº¿ï¼‰"""
    if df.empty or is_processing_cancelled():
        return df
        
    try:
        if freq == 'W-FRI':  # å‘¨çº¿ï¼Œå‘¨äº”ä¸ºå‘¨æœ«
            resampled = df.resample('W-FRI').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'amount': 'sum'
            })
        elif freq == 'ME':  # æœˆçº¿ï¼ˆä¿®å¤è­¦å‘Šï¼‰
            resampled = df.resample('ME').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'amount': 'sum'
            })
        
        # é‡æ–°è®¡ç®—MACDï¼ˆå¦‚æœæ•°æ®ä¸ä¸ºç©ºï¼‰
        if not resampled.empty and 'close' in resampled.columns and not is_processing_cancelled():
            macd, signal, histogram = calculate_macd(resampled['close'], 
                                                   MACD_PARAMS['fast'], 
                                                   MACD_PARAMS['slow'], 
                                                   MACD_PARAMS['signal'])
            resampled['macd'] = macd
            resampled['macd_signal'] = signal
            resampled['macd_histogram'] = histogram
        
        return resampled
    except Exception as e:
        logger.error(f"é‡é‡‡æ ·æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return df

def get_last_data_date(stock_code, table_name):
    """è·å–æ•°æ®åº“ä¸­è¯¥è‚¡ç¥¨çš„æœ€æ–°æ•°æ®æ—¥æœŸ"""
    if is_processing_cancelled():
        return None
        
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute(f"SELECT MAX(date) FROM {table_name} WHERE stock_code = %s", (stock_code,))
            result = cur.fetchone()
            return result[0] if result and result[0] else None
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ {stock_code} æœ€æ–°æ•°æ®æ—¥æœŸæ—¶å‡ºé”™: {str(e)}")
        return None
    finally:
        if conn:
            put_db_connection(conn)

def save_to_database(stock_code, daily_df, weekly_df, monthly_df):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
    if is_processing_cancelled():
        return False
        
    conn = None
    try:
        conn = get_db_connection()
        
        with conn.cursor() as cur:
            # æ‰¹é‡æ’å…¥æ—¥çº¿æ•°æ®
            if daily_df is not None and not daily_df.empty and not is_processing_cancelled():
                daily_data = []
                for _, row in daily_df.iterrows():
                    if is_processing_cancelled():
                        return False
                    daily_data.append((
                        stock_code, 
                        row['date'], 
                        safe_float_convert(row['open']), 
                        safe_float_convert(row['high']), 
                        safe_float_convert(row['low']), 
                        safe_float_convert(row['close']),
                        safe_int_convert(row['volume']), 
                        safe_float_convert(row.get('amount', 0)),
                        safe_float_convert(row.get('macd', 0)), 
                        safe_float_convert(row.get('macd_signal', 0)), 
                        safe_float_convert(row.get('macd_histogram', 0))
                    ))
                
                cur.executemany("""
                    INSERT INTO daily_data (
                        stock_code, date, open, high, low, close, volume, amount,
                        macd, macd_signal, macd_histogram
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (stock_code, date) DO UPDATE SET
                        open = EXCLUDED.open,
                        high = EXCLUDED.high,
                        low = EXCLUDED.low,
                        close = EXCLUDED.close,
                        volume = EXCLUDED.volume,
                        amount = EXCLUDED.amount,
                        macd = EXCLUDED.macd,
                        macd_signal = EXCLUDED.macd_signal,
                        macd_histogram = EXCLUDED.macd_histogram
                """, daily_data)
            
            # æ‰¹é‡æ’å…¥å‘¨çº¿æ•°æ®
            if weekly_df is not None and not weekly_df.empty and not is_processing_cancelled():
                weekly_data = []
                for _, row in weekly_df.iterrows():
                    if is_processing_cancelled():
                        return False
                    weekly_data.append((
                        stock_code, 
                        row['date'], 
                        safe_float_convert(row['open']), 
                        safe_float_convert(row['high']), 
                        safe_float_convert(row['low']), 
                        safe_float_convert(row['close']),
                        safe_int_convert(row['volume']), 
                        safe_float_convert(row.get('amount', 0)),
                        safe_float_convert(row.get('macd', 0)), 
                        safe_float_convert(row.get('macd_signal', 0)), 
                        safe_float_convert(row.get('macd_histogram', 0))
                    ))
                
                cur.executemany("""
                    INSERT INTO weekly_data (
                        stock_code, date, open, high, low, close, volume, amount,
                        macd, macd_signal, macd_histogram
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (stock_code, date) DO UPDATE SET
                        open = EXCLUDED.open,
                        high = EXCLUDED.high,
                        low = EXCLUDED.low,
                        close = EXCLUDED.close,
                        volume = EXCLUDED.volume,
                        amount = EXCLUDED.amount,
                        macd = EXCLUDED.macd,
                        macd_signal = EXCLUDED.macd_signal,
                        macd_histogram = EXCLUDED.macd_histogram
                """, weekly_data)
            
            # æ‰¹é‡æ’å…¥æœˆçº¿æ•°æ®
            if monthly_df is not None and not monthly_df.empty and not is_processing_cancelled():
                monthly_data = []
                for _, row in monthly_df.iterrows():
                    if is_processing_cancelled():
                        return False
                    monthly_data.append((
                        stock_code, 
                        row['date'], 
                        safe_float_convert(row['open']), 
                        safe_float_convert(row['high']), 
                        safe_float_convert(row['low']), 
                        safe_float_convert(row['close']),
                        safe_int_convert(row['volume']), 
                        safe_float_convert(row.get('amount', 0)),
                        safe_float_convert(row.get('macd', 0)), 
                        safe_float_convert(row.get('macd_signal', 0)), 
                        safe_float_convert(row.get('macd_histogram', 0))
                    ))
                
                cur.executemany("""
                    INSERT INTO monthly_data (
                        stock_code, date, open, high, low, close, volume, amount,
                        macd, macd_signal, macd_histogram
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (stock_code, date) DO UPDATE SET
                        open = EXCLUDED.open,
                        high = EXCLUDED.high,
                        low = EXCLUDED.low,
                        close = EXCLUDED.close,
                        volume = EXCLUDED.volume,
                        amount = EXCLUDED.amount,
                        macd = EXCLUDED.macd,
                        macd_signal = EXCLUDED.macd_signal,
                        macd_histogram = EXCLUDED.macd_histogram
                """, monthly_data)
            
            conn.commit()
            return True
            
    except Exception as e:
        logger.error(f"ä¿å­˜è‚¡ç¥¨ {stock_code} æ•°æ®åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            put_db_connection(conn)

def process_single_stock(txt_path, progress_callback=None):
    """å¤„ç†å•ä¸ªè‚¡ç¥¨æ–‡ä»¶çš„å®Œæ•´æµç¨‹"""
    if is_processing_cancelled():
        return False, f"å¤„ç†è¢«å–æ¶ˆ: {os.path.basename(txt_path)}"
        
    try:
        stock_code = os.path.basename(txt_path).replace('.txt', '')
        
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        debug_info = []
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(txt_path):
            return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {txt_path}"
            
        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        try:
            file_mod_time = datetime.fromtimestamp(os.path.getmtime(txt_path))
            debug_info.append(f"æ–‡ä»¶ä¿®æ”¹æ—¶é—´: {file_mod_time}")
        except Exception as e:
            return False, f"æ— æ³•è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´ {txt_path}: {str(e)}"
            
        # è·å–æ•°æ®åº“æœ€æ–°æ•°æ®æ—¶é—´
        try:
            last_db_date = get_last_data_date(stock_code, 'daily_data')
            debug_info.append(f"æ•°æ®åº“æœ€æ–°æ•°æ®æ—¶é—´: {last_db_date}")
        except Exception as e:
            return False, f"æ— æ³•è·å–æ•°æ®åº“æœ€æ–°æ•°æ®æ—¥æœŸ {stock_code}: {str(e)}"
        
        # è®°å½•æ¯”è¾ƒç»“æœ
        debug_info.append(f"æ–‡ä»¶æ—¶é—´: {file_mod_time.date()}, æ•°æ®åº“æ—¶é—´: {last_db_date}")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
        needs_update = False
        update_reason = ""
        
        if last_db_date is None:
            needs_update = True
            update_reason = "æ•°æ®åº“ä¸­æ— è¯¥è‚¡ç¥¨æ•°æ®"
            debug_info.append("éœ€è¦æ›´æ–°: æ•°æ®åº“ä¸­æ— è¯¥è‚¡ç¥¨æ•°æ®")
        elif file_mod_time.date() > last_db_date:
            needs_update = True
            update_reason = f"æ–‡ä»¶æ›´æ–° ({file_mod_time.date()} > {last_db_date})"
            debug_info.append("éœ€è¦æ›´æ–°: æ–‡ä»¶æ¯”æ•°æ®åº“æ–°")
        else:
            update_reason = f"æ— éœ€æ›´æ–° (æ–‡ä»¶æ—¶é—´ {file_mod_time.date()} <= æ•°æ®åº“æ—¶é—´ {last_db_date})"
            debug_info.append("æ— éœ€æ›´æ–°: æ–‡ä»¶ä¸æ¯”æ•°æ®åº“æ–°")
        
        debug_info.append(f"æ›´æ–°åˆ¤æ–­: {update_reason}")
        
        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰è¯¥è‚¡ç¥¨æ•°æ®ï¼Œæˆ–è€…æ–‡ä»¶æ›´æ–°æ—¶é—´æ™šäºæ•°æ®åº“æœ€æ–°æ•°æ®ï¼Œåˆ™å¤„ç†
        if needs_update:
            # å¤„ç†TXTæ–‡ä»¶ç”Ÿæˆä¸‰ç§å‘¨æœŸæ•°æ®
            try:
                daily_df, weekly_df, monthly_df = process_txt_file(txt_path, last_db_date)
                debug_info.append(f"æ•°æ®å¤„ç†ç»“æœ: daily_df={daily_df is not None}, rows={len(daily_df) if daily_df is not None else 0}")
            except Exception as e:
                debug_info.append(f"æ•°æ®å¤„ç†å¼‚å¸¸: {str(e)}")
                return False, f"å¤„ç†TXTæ–‡ä»¶å¤±è´¥ {stock_code}: {str(e)} åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
            
            if is_processing_cancelled():
                return False, f"å¤„ç†è¢«å–æ¶ˆ: {stock_code} åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
                
            if daily_df is None or len(daily_df) == 0:
                return False, f"è‚¡ç¥¨ {stock_code} æ•°æ®å¤„ç†å¤±è´¥: æœªç”Ÿæˆæœ‰æ•ˆæ•°æ® åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            try:
                success = save_to_database(stock_code, daily_df, weekly_df, monthly_df)
                debug_info.append(f"æ•°æ®åº“ä¿å­˜ç»“æœ: {success}")
            except Exception as e:
                debug_info.append(f"æ•°æ®åº“ä¿å­˜å¼‚å¸¸: {str(e)}")
                return False, f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ {stock_code}: {str(e)} åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
            
            if is_processing_cancelled():
                return False, f"å¤„ç†è¢«å–æ¶ˆ: {stock_code} åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
                
            if success:
                return True, f"è‚¡ç¥¨ {stock_code} å¤„ç†æˆåŠŸ åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
            else:
                return False, f"è‚¡ç¥¨ {stock_code} æ•°æ®åº“ä¿å­˜å¤±è´¥ åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
        else:
            return True, f"è‚¡ç¥¨ {stock_code} {update_reason} åŸå§‹ä¿¡æ¯: {'; '.join(debug_info)}"
            
    except Exception as e:
        error_msg = f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        logger.error(error_msg)
        return False, error_msg

def get_all_table_names():
    """è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨å"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
            """)
            return [row[0] for row in cur.fetchall()]
    except Exception as e:
        logger.error(f"è·å–è¡¨ååˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return []
    finally:
        if conn:
            put_db_connection(conn)

def clear_database():
    """æ¸…ç©ºæ•°æ®åº“æ‰€æœ‰è¡¨"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # è·å–æ‰€æœ‰è¡¨å
            tables = get_all_table_names()
            for table in tables:
                cur.execute(f"TRUNCATE TABLE {table} RESTART IDENTITY CASCADE")
            conn.commit()
            return True, f"æˆåŠŸæ¸…ç©º {len(tables)} ä¸ªè¡¨"
    except Exception as e:
        error_msg = f"æ¸…ç©ºæ•°æ®åº“æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if conn:
            conn.rollback()
        return False, error_msg
    finally:
        if conn:
            put_db_connection(conn)

def drop_table(table_name):
    """åˆ é™¤æŒ‡å®šè¡¨"""
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute(f"DROP TABLE IF EXISTS {table_name} CASCADE")
            conn.commit()
            return True, f"æˆåŠŸåˆ é™¤è¡¨ {table_name}"
    except Exception as e:
        error_msg = f"åˆ é™¤è¡¨ {table_name} æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        if conn:
            conn.rollback()
        return False, error_msg
    finally:
        if conn:
            put_db_connection(conn)

def batch_process_stocks(txt_dir, progress_callback=None, log_callback=None):
    """æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨æ–‡ä»¶"""
    global processing_cancelled
    reset_cancel_flag()  # é‡ç½®å–æ¶ˆæ ‡å¿—
    
    logger.info("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†è‚¡ç¥¨æ•°æ®...")
    start_time = time.time()
    
    try:
        # è·å–æ‰€æœ‰TXTæ–‡ä»¶
        if not os.path.exists(txt_dir):
            error_msg = f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {txt_dir}"
            if log_callback:
                log_callback("âŒ " + error_msg)
            return False, error_msg
            
        txt_files = [
            os.path.join(txt_dir, f) 
            for f in os.listdir(txt_dir) 
            if f.endswith('.txt')
        ]
        
        logger.info(f"ğŸ“‹ å‘ç° {len(txt_files)} ä¸ªTXTæ–‡ä»¶å¾…å¤„ç†")
        
        if not txt_files:
            if log_callback:
                log_callback("âš ï¸ æ²¡æœ‰æ‰¾åˆ°TXTæ–‡ä»¶")
            return False, "æ²¡æœ‰æ‰¾åˆ°TXTæ–‡ä»¶"
        
        # åˆ†æ‰¹å¤„ç†
        total_files = len(txt_files)
        successful = 0
        failed_files = []
        completed = 0
        last_progress_update = 0
        UPDATE_INTERVAL = 5  # æ¯5%æ›´æ–°ä¸€æ¬¡è¿›åº¦
        
        # ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†
        with ThreadPoolExecutor(max_workers=PROCESSING_CONFIG['MAX_WORKERS']) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(process_single_stock, txt_path): txt_path 
                for txt_path in txt_files
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                if is_processing_cancelled():
                    if log_callback:
                        log_callback("âŒ å¤„ç†å·²è¢«ç”¨æˆ·å–æ¶ˆ")
                    break
                    
                txt_path = future_to_file[future]
                try:
                    # æ·»åŠ è¶…æ—¶æ§åˆ¶
                    success, message = future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
                    if success:
                        successful += 1
                    else:
                        failed_files.append((os.path.basename(txt_path), message))
                    
                    completed += 1
                    # æ›´æ–°è¿›åº¦ï¼ˆå‡å°‘æ›´æ–°é¢‘ç‡ï¼‰
                    if progress_callback and total_files > 0:
                        current_progress = int((completed / total_files) * 100)
                        if current_progress - last_progress_update >= UPDATE_INTERVAL or current_progress == 100:
                            progress_callback(current_progress)
                            last_progress_update = current_progress
                    
                    # è®°å½•æ—¥å¿—
                    if log_callback:
                        log_callback(message)
                        
                    # å®šæœŸæ¸…ç†å†…å­˜
                    if successful % PROCESSING_CONFIG['MEMORY_CLEANUP_INTERVAL'] == 0:
                        gc.collect()
                        
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶ {txt_path} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
                    failed_files.append((os.path.basename(txt_path), error_msg))
                    if log_callback:
                        log_callback("âŒ " + error_msg)
                        
                # å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                if is_processing_cancelled():
                    if log_callback:
                        log_callback("âŒ å¤„ç†å·²è¢«ç”¨æˆ·å–æ¶ˆ")
                    break
    
    except Exception as e:
        error_msg = f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        if log_callback:
            log_callback("âŒ " + error_msg)
        logger.error(error_msg)
        return False, error_msg
    
    # ç»Ÿè®¡ç»“æœ
    total_time = time.time() - start_time
    if is_processing_cancelled():
        result_msg = f"âš ï¸ å¤„ç†è¢«å–æ¶ˆ! å·²å¤„ç†: {completed}/{total_files}, æˆåŠŸ: {successful}, è€—æ—¶: {total_time:.2f}ç§’"
    else:
        result_msg = f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ! æ€»æ–‡ä»¶æ•°: {total_files}, æˆåŠŸ: {successful}, å¤±è´¥: {len(failed_files)}, è€—æ—¶: {total_time:.2f}ç§’"
    
    if log_callback:
        log_callback(result_msg)
        if failed_files and not is_processing_cancelled():
            log_callback("âŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for filename, error in failed_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                log_callback(f"  - {filename}: {error}")
            if len(failed_files) > 10:
                log_callback(f"  ... è¿˜æœ‰ {len(failed_files) - 10} ä¸ªé”™è¯¯æ–‡ä»¶")
    
    return not is_processing_cancelled(), result_msg

# åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± å’Œåˆ›å»ºè¡¨
if __name__ != "__main__":
    initialize_db_pool()
    create_tables_if_not_exists()